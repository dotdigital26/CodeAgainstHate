{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CodeAgainstHateV2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2z1OIJ2tZz0j22vAIQ5iX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotdigital26/CodeAgainstHate/blob/main/CodeAgainstHateV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb5LvgXURV22",
        "outputId": "a36fde75-143e-4feb-d8d5-0f18b650255c"
      },
      "source": [
        "##Loading Libraries for model\n",
        "import random\n",
        "\n",
        "SEED = 32\n",
        "random.seed(SEED)\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report\n",
        "from sklearn.metrics import  f1_score\n",
        "\n",
        "from torch import nn\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torch.nn  import functional as F\n",
        "import torch.optim as  optim \n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "  print(\"gpu up\")\n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#train split and fit models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3U1iKryYiC4",
        "outputId": "3e495677-9e70-47b6-e2c7-8cde0090aa92"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from nltk.stem  import PorterStemmer\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stops = stopwords.words(\"english\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqz_TNgCfHq_"
      },
      "source": [
        "def removepunc(my_str): # function to remove punctuation\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in my_str:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "    return no_punct\n",
        "\n",
        "def hasNumbers(inputString):\n",
        "    return bool(re.search(r'\\d', inputString))\n",
        "snowstem = SnowballStemmer(\"english\")\n",
        "portstem = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F5CIdvrtRiXQ",
        "outputId": "681a4a1f-9ea3-4eff-99b9-eeda82650003"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c90376b-df1f-47be-8a5a-10547bfb5d8f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c90376b-df1f-47be-8a5a-10547bfb5d8f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_with_solutions[1].csv to test_with_solutions[1].csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz4Z2R3CRa_T"
      },
      "source": [
        "train_data =pd.read_csv('train[1].csv')\n",
        "train_data.drop(\"Date\",axis=1,inplace=True)\n",
        "test_data =pd.read_csv('test_with_solutions[1].csv')\n",
        "test_data.drop([\"Date\",\"Usage\"],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "gHNwlZW0vd3W",
        "outputId": "478f3df4-e76f-4546-e4ef-74b0b37c8018"
      },
      "source": [
        "train_data['Insult'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f98ba851850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCZza8gcS_uk"
      },
      "source": [
        "def myTokenizer(x):\n",
        " return  [snowstem.stem(word.text)for word in \n",
        "          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n",
        "          if (word.text not in stops and not hasNumbers(word.text)) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znKytl7IcWEx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aArb15SSVSiQ"
      },
      "source": [
        "TEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
        "LABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n",
        "class DataFrameDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
        "        fields = [('Comment', text_field), ('Insult', label_field)]\n",
        "        examples = []\n",
        "        for i, row in df.iterrows():\n",
        "            label = row.Insult \n",
        "            text = row.Comment\n",
        "            examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "  \n",
        "\n",
        "torchdataset = DataFrameDataset(train_data, TEXT,LABEL)\n",
        "torchtest = DataFrameDataset(test_data, TEXT,LABEL)\n",
        "train_data, valid_data = torchdataset.split(split_ratio=0.9, random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiWYb8eoctoM"
      },
      "source": [
        "TEXT.build_vocab(train_data)  \n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj138Nooc6w7",
        "outputId": "4ec136bb-c273-4850-b581-b006b17a3884"
      },
      "source": [
        "TEXT.build_vocab(train_data)  \n",
        "LABEL.build_vocab(train_data)\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(15)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 10534\n",
            "Size of LABEL vocabulary: 2\n",
            "[('n', 1428), ('like', 687), ('get', 453), ('dont', 421), ('fuck', 407), ('go', 379), ('peopl', 371), ('one', 342), ('would', 330), ('know', 320), ('think', 316), ('make', 276), (' ', 253), ('say', 252), ('time', 251)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or8Fte1rhHUB"
      },
      "source": [
        "#set batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\"\"\"\n",
        "we are using batches for validation and test set because of memory usage we can't pass the whole set at once \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
        "    (train_data,valid_data,torchtest), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort =False,\n",
        "shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiJyb_vHhnYo",
        "outputId": "59737cce-f148-4dcf-9136-1d17e8ace374"
      },
      "source": [
        "class TextTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextTransformer,self).__init__()\n",
        "    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n",
        "    self.positionEmbeddings = nn.Embedding(140,20)\n",
        "    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n",
        "    self.linear1 = nn.Linear(160,  64)\n",
        "    self.linear2 = nn.Linear(64,  1)\n",
        "    self.linear3 = nn.Linear(140,  16)\n",
        "    self.linear4 = nn.Linear(16,  1)\n",
        "  def forward(self,x):\n",
        "    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n",
        "    # broadcasting the tensor of positions \n",
        "    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n",
        "    attended = self.transformerLayer(sentence)\n",
        "    linear1 = F.relu(self.linear1(attended))\n",
        "    linear2 = F.relu(self.linear2(linear1))\n",
        "    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
        "    linear3 = F.relu(self.linear3(linear2))\n",
        "    out = torch.sigmoid(self.linear4(linear3))\n",
        "    return out\n",
        "\n",
        "myTransformer = TextTransformer()\n",
        "myTransformer.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextTransformer(\n",
              "  (wordEmbeddings): Embedding(10534, 140)\n",
              "  (positionEmbeddings): Embedding(140, 20)\n",
              "  (transformerLayer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): _LinearWithBias(in_features=160, out_features=160, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=160, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=160, bias=True)\n",
              "    (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (linear1): Linear(in_features=160, out_features=64, bias=True)\n",
              "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (linear3): Linear(in_features=140, out_features=16, bias=True)\n",
              "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FORCADCbjJ42"
      },
      "source": [
        "def calculateMetrics(ypred,ytrue):\n",
        "  acc  = accuracy_score(ytrue,ypred)\n",
        "  f1  = f1_score(ytrue,ypred)\n",
        "  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n",
        "  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xymTYPa9kktE",
        "outputId": "6d2f42e4-0d62-4dc0-dd2f-de70dc431ec1"
      },
      "source": [
        "\n",
        "optimizer = optim.Adam(myTransformer.parameters(),lr = 0.001)\n",
        "\n",
        "for i in range(20):\n",
        "  trainpreds = torch.tensor([])\n",
        "  traintrues = torch.tensor([])\n",
        "  for  batch in train_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    myTransformer.zero_grad()\n",
        "    pred = myTransformer(X).squeeze()\n",
        "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
        "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
        "    err = F.binary_cross_entropy(pred,y)\n",
        "    err.backward()\n",
        "    optimizer.step()\n",
        "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
        "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
        " \n",
        "\n",
        "  valpreds = torch.tensor([])\n",
        "  valtrues = torch.tensor([])\n",
        "  for batch in valid_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
        "    pred = myTransformer(X).squeeze().cpu().detach()\n",
        "    # print(valtrues.shape)\n",
        "    valpreds = torch.cat((valpreds,pred))\n",
        "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
        "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train BCE loss:  0.6610249280929565  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6509506106376648  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6474252939224243  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.636728048324585  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6342032551765442  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6223083734512329  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6212819218635559  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6087058186531067  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6096282005310059  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5964187979698181  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5996088981628418  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.586143434047699  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.591873288154602  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5784144401550293  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5865882635116577  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.573193371295929  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5834144353866577  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5700058341026306  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5817413926124573  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5682170987129211  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5809628367424011  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5672694444656372  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5806388258934021  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5667812824249268  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5805158615112305  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5665305256843567  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804722309112549  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5664010643959045  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804574489593506  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5663337707519531  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.580452561378479  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662989020347595  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804510116577148  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662810802459717  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804505944252014  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662721991539001  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804505944252014  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662679672241211  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.580450713634491  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662660002708435  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw3pOS7KpYOv",
        "outputId": "a5e79240-54b6-4eca-890a-47b22eea22df"
      },
      "source": [
        "\n",
        "optimizer = optim.Adagrad(myTransformer.parameters(),lr = 0.001)\n",
        "\n",
        "for i in range(20):\n",
        "  trainpreds = torch.tensor([])\n",
        "  traintrues = torch.tensor([])\n",
        "  for  batch in train_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    myTransformer.zero_grad()\n",
        "    pred = myTransformer(X).squeeze()\n",
        "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
        "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
        "    err = F.binary_cross_entropy(pred,y)\n",
        "    err.backward()\n",
        "    optimizer.step()\n",
        "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
        "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
        " \n",
        "\n",
        "  valpreds = torch.tensor([])\n",
        "  valtrues = torch.tensor([])\n",
        "  for batch in valid_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
        "    pred = myTransformer(X).squeeze().cpu().detach()\n",
        "    # print(valtrues.shape)\n",
        "    valpreds = torch.cat((valpreds,pred))\n",
        "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
        "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train BCE loss:  0.5804764628410339  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661464333534241  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804509520530701  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661569833755493  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804464221000671  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661646127700806  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804438591003418  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661706328392029  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804420709609985  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661754608154297  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804407000541687  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661795735359192  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804398059844971  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661831498146057  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804389119148254  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.566186249256134  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804381966590881  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661890506744385  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804376006126404  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.566191554069519  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804370641708374  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661937594413757  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804367065429688  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661959052085876  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804362893104553  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661976337432861  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804359316825867  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5661994218826294  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804356932640076  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662009716033936  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804353356361389  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662024617195129  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.580435037612915  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662038922309875  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804348587989807  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662052631378174  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804346203804016  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662063956260681  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804344415664673  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5662075281143188  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVwHLzZdukuu",
        "outputId": "b3d1927f-9f17-4af1-e982-a62aaeb5f639"
      },
      "source": [
        "\"\"\"\n",
        "now getting the results on the test set\n",
        "\"\"\"\n",
        "\n",
        "testpreds = torch.tensor([])\n",
        "testtrues = torch.tensor([])\n",
        "for batch in test_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    testtrues = torch.cat((testtrues,y.cpu().detach()))\n",
        "    pred = myTransformer(X).squeeze().cpu().detach()\n",
        "    # print(valtrues.shape)\n",
        "    testpreds = torch.cat((testpreds,pred))\n",
        "err = F.binary_cross_entropy(testpreds,testtrues)\n",
        "print(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test BCE loss:  0.5749812126159668  f1 score: 0.0 f1 average: 0.425 accuracy: 0.738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "PTdPkXvbu2vV",
        "outputId": "eff51bd1-036e-473f-d32a-38d9e0cffdad"
      },
      "source": [
        "test_data[\"predicted\"] = torch.round(testpreds).numpy()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "this shows that the model understands the language well \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "test_data[test_data.predicted==0].iloc[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Comment</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>\"@ian21\\xa0\"Roger Clemens is the fucking man, ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Agree with Alan you are an extremest idiot.  ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Really suck isn't the word, when many of our ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Insult                                            Comment  predicted\n",
              "1       0  \"@ian21\\xa0\"Roger Clemens is the fucking man, ...        0.0\n",
              "2       1  \"Agree with Alan you are an extremest idiot.  ...        0.0\n",
              "3       0  \"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...        0.0\n",
              "4       0  \"Really suck isn't the word, when many of our ...        0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lEb6_3pyeOr"
      },
      "source": [
        "Dealing With Imbalanced Data set using Undersampling of the majority class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tcxzm_8ybNn",
        "outputId": "74d1fc14-1f94-4d49-ae66-69b70216f191"
      },
      "source": [
        "#CODE TO Undersample\n",
        "#u = train_data.columns[:]\n",
        "\n",
        "##Class_count\n",
        "count_class_0, count_class_1 = train_data.Insult.value_counts()\n",
        "\n",
        "##Divide by Class\n",
        "df_class_0 = train_data[train_data ['Insult']==0]\n",
        "df_class_1 = train_data[train_data ['Insult']==1]\n",
        "\n",
        "##Oversampling of Imbalance_class\n",
        "df_class_0_over = df_class_0.sample(count_class_1, replace =True)\n",
        "df_class_merge = pd.concat([df_class_1, df_class_0_over], axis=0)\n",
        "df_class_merge.Insult.value_counts()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1049\n",
              "0    1049\n",
              "Name: Insult, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM_oKzs8-3tv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILEc7xo4yu18"
      },
      "source": [
        "def myTokenizer(x):\n",
        " return  [snowstem.stem(word.text)for word in \n",
        "          tokenizer(removepunc(re.sub(r\"\\s+\\s+\",\" \",re.sub(r\"[^A-Za-z0-9()!?\\'\\`\\\"\\r+\\n+]\",\" \",x.lower()))).strip()) \n",
        "          if (word.text not in stops and not hasNumbers(word.text)) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkyruHo7y5N4"
      },
      "source": [
        "TEXT = data.Field(tokenize=myTokenizer,batch_first=True,fix_length=140)\n",
        "LABEL = data.LabelField(dtype=torch.float ,batch_first=True)\n",
        "class DataFrameDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
        "        fields = [('Comment', text_field), ('Insult', label_field)]\n",
        "        examples = []\n",
        "        for i, row in df.iterrows():\n",
        "            label = row.Insult \n",
        "            text = row.Comment\n",
        "            examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "  \n",
        "\n",
        "torchdataset = DataFrameDataset(train_data, TEXT,LABEL)\n",
        "torchtest = DataFrameDataset(test_data, TEXT,LABEL)\n",
        "train_data, valid_data = torchdataset.split(split_ratio=0.9, random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb_xJSAJzFBp",
        "outputId": "06965554-d432-4f26-aa6d-299c500fa36c"
      },
      "source": [
        "TEXT.build_vocab(train_data)  \n",
        "LABEL.build_vocab(train_data)\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(15)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 10534\n",
            "Size of LABEL vocabulary: 2\n",
            "[('n', 1428), ('like', 687), ('get', 453), ('dont', 421), ('fuck', 407), ('go', 379), ('peopl', 371), ('one', 342), ('would', 330), ('know', 320), ('think', 316), ('make', 276), (' ', 253), ('say', 252), ('time', 251)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLhlfc6gzNMQ"
      },
      "source": [
        "#set batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\"\"\"\n",
        "we are using batches for validation and test set because of memory usage we can't pass the whole set at once \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
        "    (train_data,valid_data,torchtest), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort =False,\n",
        "shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXINrqAYzTcC",
        "outputId": "3f9d10d1-584e-4442-9ef2-4e3d01d8e4e7"
      },
      "source": [
        "class TextTransformer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextTransformer,self).__init__()\n",
        "    self.wordEmbeddings = nn.Embedding(len(TEXT.vocab),140)\n",
        "    self.positionEmbeddings = nn.Embedding(140,20)\n",
        "    self.transformerLayer = nn.TransformerEncoderLayer(160,8) \n",
        "    self.linear1 = nn.Linear(160,  64)\n",
        "    self.linear2 = nn.Linear(64,  1)\n",
        "    self.linear3 = nn.Linear(140,  16)\n",
        "    self.linear4 = nn.Linear(16,  1)\n",
        "  def forward(self,x):\n",
        "    positions = (torch.arange(0,140).reshape(1,140) + torch.zeros(x.shape[0],140)).to(device) \n",
        "    # broadcasting the tensor of positions \n",
        "    sentence = torch.cat((self.wordEmbeddings(x.long()),self.positionEmbeddings(positions.long())),axis=2)\n",
        "    attended = self.transformerLayer(sentence)\n",
        "    linear1 = F.relu(self.linear1(attended))\n",
        "    linear2 = F.relu(self.linear2(linear1))\n",
        "    linear2 = linear2.view(-1,140) # reshaping the layer as the transformer outputs a 2d tensor (or 3d considering the batch size)\n",
        "    linear3 = F.relu(self.linear3(linear2))\n",
        "    out = torch.sigmoid(self.linear4(linear3))\n",
        "    return out\n",
        "\n",
        "myTransformer = TextTransformer()\n",
        "myTransformer.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextTransformer(\n",
              "  (wordEmbeddings): Embedding(10534, 140)\n",
              "  (positionEmbeddings): Embedding(140, 20)\n",
              "  (transformerLayer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): _LinearWithBias(in_features=160, out_features=160, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=160, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=160, bias=True)\n",
              "    (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (linear1): Linear(in_features=160, out_features=64, bias=True)\n",
              "  (linear2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (linear3): Linear(in_features=140, out_features=16, bias=True)\n",
              "  (linear4): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfelAz-Wzbma"
      },
      "source": [
        "def calculateMetrics(ypred,ytrue):\n",
        "  acc  = accuracy_score(ytrue,ypred)\n",
        "  f1  = f1_score(ytrue,ypred)\n",
        "  f1_average  = f1_score(ytrue,ypred,average=\"macro\")\n",
        "  return \" f1 score: \"+str(round(f1,3))+\" f1 average: \"+str(round(f1_average,3))+\" accuracy: \"+str(round(acc,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tulQPKeOzjR_",
        "outputId": "b18a475f-786b-41ff-d2f3-81fdccee0291"
      },
      "source": [
        "\n",
        "optimizer = optim.Adam(myTransformer.parameters(),lr = 0.001)\n",
        "\n",
        "for i in range(20):\n",
        "  trainpreds = torch.tensor([])\n",
        "  traintrues = torch.tensor([])\n",
        "  for  batch in train_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    myTransformer.zero_grad()\n",
        "    pred = myTransformer(X).squeeze()\n",
        "    trainpreds = torch.cat((trainpreds,pred.cpu().detach()))\n",
        "    traintrues = torch.cat((traintrues,y.cpu().detach()))\n",
        "    err = F.binary_cross_entropy(pred,y)\n",
        "    err.backward()\n",
        "    optimizer.step()\n",
        "  err = F.binary_cross_entropy(trainpreds,traintrues)\n",
        "  print(\"train BCE loss: \",err.item(),calculateMetrics(torch.round(trainpreds).numpy(),traintrues.numpy()))\n",
        " \n",
        "\n",
        "  valpreds = torch.tensor([])\n",
        "  valtrues = torch.tensor([])\n",
        "  for batch in valid_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    valtrues = torch.cat((valtrues,y.cpu().detach()))\n",
        "    pred = myTransformer(X).squeeze().cpu().detach()\n",
        "    # print(valtrues.shape)\n",
        "    valpreds = torch.cat((valpreds,pred))\n",
        "  err = F.binary_cross_entropy(valpreds,valtrues)\n",
        "  print(\"validation BCE loss: \",err.item(),calculateMetrics(torch.round(valpreds).numpy(),valtrues.numpy()))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train BCE loss:  0.7048015594482422  f1 score: 0.422 f1 average: 0.211 accuracy: 0.267\n",
            "validation BCE loss:  0.6966125965118408  f1 score: 0.404 f1 average: 0.202 accuracy: 0.253\n",
            "train BCE loss:  0.6902215480804443  f1 score: 0.248 f1 average: 0.492 accuracy: 0.61\n",
            "validation BCE loss:  0.6829929947853088  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6773678660392761  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.668710470199585  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6634654402732849  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6533322930335999  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6489499807357788  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6376236081123352  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6346308588981628  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.622464120388031  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6213372945785522  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.6086786389350891  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6097609996795654  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5968886017799377  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.6003352403640747  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5874185562133789  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5931802988052368  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5802749395370483  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5881279110908508  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5752041935920715  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5848120450973511  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5718005299568176  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.582787811756134  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5696226954460144  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5816351175308228  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5682787895202637  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5810201168060303  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5674688816070557  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5807106494903564  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5669854879379272  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5805631279945374  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5666971802711487  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804958939552307  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5665239691734314  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804663896560669  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.566419243812561  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n",
            "train BCE loss:  0.5804539322853088  f1 score: 0.0 f1 average: 0.423 accuracy: 0.733\n",
            "validation BCE loss:  0.5663553476333618  f1 score: 0.0 f1 average: 0.428 accuracy: 0.747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwOgUSfP50pB",
        "outputId": "4abd2d28-e078-4709-cdaa-aeb22f7e4442"
      },
      "source": [
        "\"\"\"\n",
        "now getting the results on the test set\n",
        "\"\"\"\n",
        "\n",
        "testpreds = torch.tensor([])\n",
        "testtrues = torch.tensor([])\n",
        "for batch in test_iterator:\n",
        "    X = batch.Comment\n",
        "    y = batch.Insult\n",
        "    testtrues = torch.cat((testtrues,y.cpu().detach()))\n",
        "    pred = myTransformer(X).squeeze().cpu().detach()\n",
        "    # print(valtrues.shape)\n",
        "    testpreds = torch.cat((testpreds,pred))\n",
        "err = F.binary_cross_entropy(testpreds,testtrues)\n",
        "print(\"test BCE loss: \",err.item(),calculateMetrics(torch.round(testpreds).numpy(),testtrues.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test BCE loss:  1.1869328022003174  f1 score: 0.42 f1 average: 0.6 accuracy: 0.681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "QxrG3O7H-5sS",
        "outputId": "1c2b08f0-a353-4a9c-b0b4-624312aec99b"
      },
      "source": [
        "test_data[\"predicted\"] = torch.round(testpreds).numpy()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "this shows that the model understands the language well \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "test_data[test_data.predicted==1].iloc[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Comment</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Really suck isn't the word, when many of our ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Matthews mentored Perry at USC.  Bodes well...\"</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>\"How about we accept facts and patiently wait ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>\"I\\\\'m a second generation German working on m...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>\"GO SUCK_YOU DADDYS_COCK_OBAMA LOVING LIBERAL_...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>\"yeah!!!! fuck undying!\"</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>\"i live with my dad asshole!!! i anal fuked th...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Hey race baiter take a hike you are a typical...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>\"go away you trashy\\xa0 tramp\"</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Insult                                            Comment  predicted\n",
              "4        0  \"Really suck isn't the word, when many of our ...        1.0\n",
              "6        0   \"Matthews mentored Perry at USC.  Bodes well...\"        1.0\n",
              "7        0  \"How about we accept facts and patiently wait ...        1.0\n",
              "20       0  \"I\\\\'m a second generation German working on m...        1.0\n",
              "23       1  \"GO SUCK_YOU DADDYS_COCK_OBAMA LOVING LIBERAL_...        1.0\n",
              "24       0                           \"yeah!!!! fuck undying!\"        1.0\n",
              "27       1  \"i live with my dad asshole!!! i anal fuked th...        1.0\n",
              "30       1  \"Hey race baiter take a hike you are a typical...        1.0\n",
              "37       1                     \"go away you trashy\\xa0 tramp\"        1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}